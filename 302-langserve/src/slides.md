---
# See all frontmatter configurations: https://sli.dev/custom/#frontmatter-configures
# theme id or package name, see also: https://sli.dev/themes/use.html
theme: 'seriph'
# titleTemplate for the webpage, `%s` will be replaced by the page's title
titleTemplate: '%sï½œWebUP'
# some information about the slides, markdown enabled
info: |
  AGI å­¦ä¹ ç¬”è®°ï¼Œä»…ä¾›ä¸ªäººå­¦ä¹ ä½¿ç”¨
# favicon, can be a local file path or URL
favicon: https://files.codelife.cc/user-website-icon/20220523/5hyKeZxOknU2owAPvnSWD1388.png?x-oss-process=image/resize,limit_0,m_fill,w_25,h_25/quality,q_92/format,webp
# enabled pdf downloading in SPA build, can also be a custom url
download: 'https://github.com/webup/agi-talks/raw/master/pdfs/302-langserve.pdf'
# syntax highlighter, can be 'prism' or 'shiki'
highlighter: 'shiki'
# controls whether texts in slides are selectable
selectable: false
# enable slide recording, can be boolean, 'dev' or 'build'
record: 'build'
# define transition between slides
transition: fade
# default frontmatter applies to all slides
defaults:

# slide configurations
hideInToc: true
layout: cover
class: text-center
background: https://source.unsplash.com/collection/94734566/1920x1080 
---

# LangServe: Serve LangChain

LangServe Intro and Walkthrough

<div class="pt-12">
  <span @click="$slidev.nav.next" class="px-2 py-1 rounded cursor-pointer" hover="bg-white bg-opacity-10">
    Press Space for next page <carbon:arrow-right class="inline"/>
  </span>
</div>

<div class="abs-br m-6 flex gap-2">
  <a href="https://muselink.cc/zhanghaili" target="_blank" alt="Auhtor"
    class="text-xl icon-btn opacity-50 !border-none !hover:text-white">
    <mdi-lecture />
  </a>
  <a href="https://github.com/webup/agi-talks" target="_blank" alt="GitHub"
    class="text-xl icon-btn opacity-50 !border-none !hover:text-white">
    <carbon-logo-github />
  </a>
  <a href="https://github.com/webup/agi-talks/raw/master/pdfs/302-langserve.pdf" target="_blank" alt="Download"
    class="text-xl icon-btn opacity-50 !border-none !hover:text-white">
    <carbon-download />
  </a>
</div>

---
src: ../../pages/common/series.md
---

---
src: ../../pages/common/toc.md
---

---
layout: iframe
url: https://embeds.onemodel.app/d/iframe/955y5DDdxGVRezQ2ZzVL718O7iuBgmQepg1aXLS7I6JvFP793OV9GySY5UoZ
---

---
layout: iframe
url: //player.bilibili.com/player.html?aid=492239144&bvid=BV1oN41147Ak&cid=1301106610&p=1
---

---
layout: iframe-right
url: https://blog.langchain.dev/introducing-langserve/
---

# LangServe ä¹‹è¯ç”Ÿ

<Tweet id="1712526285313102091" />

---
layout: two-cols-header
---

# ä½¿ç”¨ LangServe æ„å»ºç”Ÿäº§å¯ç”¨çš„ Web API

Building Production-Ready Web APIs with LangServe

::left::

<mdi-code /> `my_package/chain.py`:

```py {7-}
"""A conversational retrieval chain."""

from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS

vectorstore = FAISS.from_texts(
    ["cats like fish", "dogs like sticks"],
    embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever()

model = ChatOpenAI()

chain = ConversationalRetrievalChain.from_llm(model, retriever)
```

ä»»æ„æ„å»ºä¸€ä¸ª Chainï¼ˆå®Œå…¨å¯ä»¥æ˜¯åŸºäº [LCEL](https://python.langchain.com/docs/expression_language/) æ„å»ºï¼‰

<T>Build a Chain as you like (especially via LCEL)</T>

::right::

<mdi-code /> `my_package/server.py`:

```py {8-}
#!/usr/bin/env python
"""A server for the chain above."""

from fastapi import FastAPI
from langserve import add_routes

from my_package.chain import chain

app = FastAPI(title="Retrieval App")

add_routes(app, chain)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8000)
```

é€šè¿‡ `add_routes` æ–¹æ³•æŠŠ Chain æ³¨å†Œåˆ° Web æœåŠ¡

<T>Register chain to web server via <code>add_routes</code></T>

---
layout: iframe-right
url: https://replit.com/@webup/langserve-replit-template?embed=true
---

# LangServe Replit æ¨¡æ¿

<carbon-logo-github /> [LangServe Replit Template](https://github.com/langchain-ai/langserve-replit-template)

è¯¥æ¨¡æ¿å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ LangServe å°† LangChain Expression Language <B>Runnable éƒ¨ç½²ä¸ºä¸€ç»„ HTTP ç«¯ç‚¹</B>åˆ° [Replit](https://replit.com/) ä¸Šï¼Œå¹¶æ”¯æŒæµå’Œæ‰¹å¤„ç†

<T>This template shows how to deploy a LangChain Expression Language Runnable as a set of HTTP endpoints with stream and batch support using LangServe onto Replit.</T><br/>

###### Notice

åœ¨è¿è¡Œç¤ºä¾‹å‰ï¼Œéœ€è¦é€šè¿‡å·¦ä¸‹è§’çš„ `Tools > Secrets` æ¥è®¾ç½® `OPENAI_APâ€‹â€‹I_KEY` 
<T>Need to set an <code>OPENAI_API_KEY</code> environment variable by going under <code>Tools > Secrets</code> in the bottom left corner</T>

---
layout: iframe-right
url: https://langserve-launch-example-vz4y4ooboq-uc.a.run.app/docs
---

# LangServe ç‰¹æ€§æ¦‚è§ˆ

LangServe Endpoints and Features 

ğŸ“š `//docs` é€šè¿‡ [Swagger UI](https://swagger.io/tools/swagger-ui/) å±•ç¤ºå’Œè°ƒè¯• API

<T><code>//docs</code> endpoint serves API docs with Swagger UI</T>

ğŸ“ 4 ä¸ªâ€œå†™â€ç«¯ç‚¹ç”¨äºè°ƒç”¨ Chainï¼š`/invoke`ï¼Œ`/batch`ï¼Œ`/stream`ï¼Œ`/stream_log`

<T>4 endpoints to call your chain in various approaches</T>

ğŸ” 2 ä¸ªâ€œè¯»â€ç«¯ç‚¹ç”¨äºè·å– Chain çš„è¾“å…¥è¾“å‡ºç»“æ„ï¼š`/input_schema`ï¼Œ`/output_schema`

<T>2 endpoints to retrieve <a href="https://python.langchain.com/docs/expression_language/interface#input-schema" target="_blank">input</a> and <a href="https://python.langchain.com/docs/expression_language/interface#output-schema" target="_blank">output</a> schemas (<a href="https://docs.pydantic.dev/" target="_blank">Pydantic</a> models) auto-generated from the structure of the chain</T>

ğŸ“‚ æ”¯æŒåœ¨åŒä¸€æœåŠ¡çš„ä¸åŒè·¯å¾„ä¸‹æ‰˜ç®¡å¤šä¸ª Chain

<T>Support for hosting multiple chains in the same server under separate paths, e.g. <code>/chat/invoke</code>, <code>/say/invoke</code></T>

---

# è°ƒç”¨ LangServe ç«¯ç‚¹æ¥å£çš„å¤šç§æ–¹å¼

Calling hosted chain from various clients

```py
from langserve import RemoteRunnable

pirate_chain = RemoteRunnable("https://your_url.repl.co/chat/")

pirate_chain.invoke({"question": "how are you?"})
await pirate_chain.ainvoke({"question": "how are you?"})
```

<br/>

```js
import { RemoteRunnable } from "langchain/runnables/remote"; // Introduced in LangChain.js 0.0.166+

const pirateChain = new RemoteRunnable({ url: `https://your_url.repl.co/chat/` });
const result = await pirateChain.invoke({ "question": "what did i just say my name was?" });
```

<br/>

```sh
curl --location --request POST 'https://your_url.repl.co/chat/invoke' \
--header 'Content-Type: application/json' \
--data-raw '{
    "input": { "question": "what did i just say my name was?" }
}'
```

---

# LCEL å¯¹äº LangServe çš„é‡è¦æ”¯æ’‘

The journey to build LangServe really started when LCEL and the Runnable protocol launched

###### Streaming Capacity

ğŸŒŠ ä¸€æµçš„æµå¼ä¼ è¾“æ”¯æŒï¼šä½¿ç”¨ LCEL æ—¶ï¼Œæ‚¨å¯ä»¥è·å¾—æœ€ä½³çš„é¦–æ¬¡ Token æ—¶é—´ï¼›æ­£åœ¨å°è¯•æ”¯æŒ [æµå¼ JSON è§£æå™¨](https://twitter.com/LangChainAI/status/1709690468030914584)

<T>First-class support for streaming: build your chains with LCEL you get the best possible time-to-first-token, streaming JSON parser is WIP</T>

ğŸ” æµå¼è¾“å‡ºä¸­é—´ç»“æœï¼šæ·»åŠ äº†å¯¹ [æµå¼è¾“å‡ºä¸­é—´ç»“æœ](https://python.langchain.com/docs/expression_language/interface#async-stream-intermediate-steps) çš„æ”¯æŒï¼Œå¹¶ä¸”åœ¨æ¯ä¸ª LangServe æœåŠ¡ä¸Šéƒ½å¯ç”¨

<T>Accessing intermediate results: added support for streaming intermediate results, and itâ€™s available on every LangServe server</T>

<br/>

###### Control Flow

âš¡ï¸ ä¼˜åŒ–çš„å¹¶è¡Œæ‰§è¡Œï¼šåªè¦ LCEL é“¾å…·æœ‰ [å¯ä»¥å¹¶è¡Œæ‰§è¡Œ](https://python.langchain.com/docs/expression_language/how_to/map) çš„æ­¥éª¤ï¼Œå°±ä¼šåœ¨åŒæ­¥å’Œå¼‚æ­¥æ¥å£ä¸­è‡ªåŠ¨å¹¶è¡Œæ‰§è¡Œæ­¤æ“ä½œ

<T>Optimized parallel execution: whenever LCEL chains have steps that can be executed in parallel, will do it in both sync and async interfaces</T>

ğŸ” æ”¯æŒé‡è¯•å’Œå›é€€ï¼šä¸º LCEL é“¾çš„å¢åŠ äº† [é‡è¯•å’Œå›é€€](https://python.langchain.com/docs/expression_language/how_to/fallbacks) çš„æ”¯æŒï¼›ç›®å‰æ­£åœ¨åŠªåŠ›æ·»åŠ å¯¹é‡è¯•/å›é€€çš„æµæ”¯æŒ

<T>Retries and fallbacks: added support for any part of your LCEL chain; currently working on adding streaming support for retries/fallback</T>

---
src: ../../pages/common/refs.md
---

---
src: ../../pages/common/end.md
---
