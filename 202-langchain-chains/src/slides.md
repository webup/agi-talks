---
# See all frontmatter configurations: https://sli.dev/custom/#frontmatter-configures
# theme id or package name, see also: https://sli.dev/themes/use.html
theme: 'seriph'
# titleTemplate for the webpage, `%s` will be replaced by the page's title
titleTemplate: '%sï½œWebUP'
# some information about the slides, markdown enabled
info: |
  AGI å­¦ä¹ ç¬”è®°ï¼Œä»…ä¾›ä¸ªäººå­¦ä¹ ä½¿ç”¨
# favicon, can be a local file path or URL
favicon: https://files.codelife.cc/user-website-icon/20220523/5hyKeZxOknU2owAPvnSWD1388.png?x-oss-process=image/resize,limit_0,m_fill,w_25,h_25/quality,q_92/format,webp
# enabled pdf downloading in SPA build, can also be a custom url
download: 'https://github.com/webup/agi-talks/raw/master/pdfs/202-langchain-chains.pdf'
# syntax highlighter, can be 'prism' or 'shiki'
highlighter: 'shiki'
# controls whether texts in slides are selectable
selectable: false
# enable slide recording, can be boolean, 'dev' or 'build'
record: 'build'
# define transition between slides
transition: fade
# default frontmatter applies to all slides
defaults:

# slide configurations
hideInToc: true
layout: cover
class: text-center
background: https://source.unsplash.com/collection/94734566/1920x1080 
---

# LangChain æ¨¡å—è§£æ<sub>ï¼ˆä¸­ï¼‰</sub>

LangChain Chains Hands-on

<div class="pt-12">
  <span @click="$slidev.nav.next" class="px-2 py-1 rounded cursor-pointer" hover="bg-white bg-opacity-10">
    Press Space for next page <carbon:arrow-right class="inline"/>
  </span>
</div>

<div class="abs-br m-6 flex gap-2">
  <a href="https://muselink.cc/zhanghaili" target="_blank" alt="Auhtor"
    class="text-xl icon-btn opacity-50 !border-none !hover:text-white">
    <mdi-lecture />
  </a>
  <a href="https://github.com/webup/agi-talks" target="_blank" alt="GitHub"
    class="text-xl icon-btn opacity-50 !border-none !hover:text-white">
    <carbon-logo-github />
  </a>
  <a href="https://github.com/webup/agi-talks/raw/master/pdfs/202-langchain-chains.pdf" target="_blank" alt="Download"
    class="text-xl icon-btn opacity-50 !border-none !hover:text-white">
    <carbon-download />
  </a>
</div>

---
src: ../../pages/common/series.md
---

---
hideInToc: true
---

# æ•™ç¨‹å¤§çº²

<Toc maxDepth="2" listClass="!list-disc" columns="2" />

---
layout: iframe
url: https://embeds.onemodel.app/d/iframe/cl8GryOek12pB78SZZYZCsJSJsS159pNgqjS19zQWC3tka5TXz9rqLmqXORI
hideInToc: true
---

---
layout: iframe
url: //player.bilibili.com/player.html?aid=275180495&bvid=BV1SF411C7Zn&cid=1244328447
hideInToc: true
---

---
layout: quote
---

# Runnable Sequence

LangChain Expression Language (LCEL) çš„æ ¸å¿ƒ

---
level: 2
---

# Runnable å¯¹è±¡ç®€ä»‹

æŠŠä¸€ä¸ªä¸ª Runnable å¯¹è±¡ä¸²è”èµ·æ¥ï¼Œå°±æ„æˆäº† Runnable Sequence

<p><B>Runnable å¯¹è±¡çš„å®šä¹‰</B>ï¼šæ”¯æŒä»¥ä¸‹ä¸‰ä¸ªå®ä¾‹æ–¹æ³•çš„å¯¹è±¡</p>

- `invoke`ï¼šå¯¹è¾“å…¥ç›´æ¥è¿›è¡Œé“¾å¼è°ƒç”¨ 
- `batch`ï¼šå¯¹è¾“å…¥åˆ—è¡¨è¿›è¡Œæ‰¹é‡çš„é“¾å¼è°ƒç”¨
- `stream`ï¼šï¼ˆæµå¼ï¼‰åˆ†å—è¿”å›å“åº”ï¼ˆéœ€è¦ Modelã€Parser ç­‰æ”¯æŒï¼‰

<br/>

<p><B>Runnable å¯¹è±¡çš„è¾“å…¥è¾“å‡º</B>ï¼šå„æœ‰ä¸åŒï¼Œå¹¶éç»Ÿä¸€ï¼ˆä»¥ä¸‹ä»¥ JS/TS SDK ä¸ºä¾‹ï¼‰</p>

- Promptï¼š`Object` â¡ï¸ `PromptValue`
- LLM Modelï¼š`String` â¡ï¸ `String` 
- Chat Modelï¼šä¸€ç»„ `ChatMessage` â¡ï¸ `ChatMessage`
- Parserï¼šModel çš„è¾“å‡º â¡ï¸ ä¸åŒ Parser å¯¹åº”çš„æ•°æ®ç»“æ„
- Retrieverï¼š`String` â¡ï¸ ä¸€ç»„ `Document`

---
level: 2
---

# Runnable å¯¹è±¡çš„ä¸²è”

- JS/TS ä¸­å¯ç”¨çš„å®ä¾‹æ–¹æ³•ï¼š`runnable.pipe(runnable)`
- JS/TS ä¸­å¯ç”¨çš„é™æ€æ–¹æ³•ï¼š`RunnableSequence.from([...runnables ])`

```ts {6-9|11-13|15-17|all}
import { PromptTemplate } from "langchain/prompts";
import { OpenAI } from "langchain/llm/openai";
import { RunnableSequence } from "langchain/schema/runnable";
import { StringOutputParser } from "langchain/schema/output_parser";

/* å‡†å¤‡ Runnable å¯¹è±¡ */
const model = new OpenAI({});
const promptTemplate = PromptTemplate.fromTemplate("Tell me a joke about {topic}");
const outputParser = new StringOutputParser();

/* ä¸²è” Runnable å¯¹è±¡ */
// const chain = promptTemplate.pipe(model).pipe(outputParser);
const chain = RunnableSequence.from([promptTemplate, model, outputParser]);

/* è°ƒç”¨ Runnable Sequence */
const result = await chain.invoke({ topic: "bears" });
// const result = await chain.batch([{ topic: "bears" }, { topic: "dears" }]);
```

---
layout: iframe
url: https://smith.langchain.com/public/d6e1b8d9-bd25-431b-9e5a-6b852cf1e0e0/r
---

---
level: 2
---

# Runnable å¯¹è±¡ç»‘å®šå‚æ•°

- JS/TS ä¸­å¯ç”¨çš„å®ä¾‹æ–¹æ³•ï¼š`runnable.bind( ...kwargs )`

```ts {8-28|30-36|all} {maxHeight:'80%'}
import { PromptTemplate } from "langchain/prompts";
import { ChatOpenAI } from "langchain/chat_models/openai";

/* å‡†å¤‡ Runnable å¯¹è±¡ */
const prompt = PromptTemplate.fromTemplate(`Tell me a joke about {subject}`);
const model = new ChatOpenAI({});

/* å‡†å¤‡ Runnable å¯¹è±¡çš„é™„åŠ å‚æ•° */
const functionSchema = [
  {
    name: "joke",
    description: "A joke",
    parameters: {
      type: "object",
      properties: {
        setup: {
          type: "string",
          description: "The setup for the joke",
        },
        punchline: {
          type: "string",
          description: "The punchline for the joke",
        },
      },
      required: ["setup", "punchline"],
    },
  },
];

/* ä¸²è” Runnable å¯¹è±¡ï¼ŒåŒæ—¶é™„åŠ å‚æ•° */
const chain = prompt.pipe(
  model.bind({
    functions: functionSchema,
    function_call: { name: "joke" },
  })
);

/* è°ƒç”¨ Runnable Sequence */
const result = await chain.invoke({ subject: "bears" });
```

---
layout: iframe
url: https://smith.langchain.com/public/4f429737-3bf8-4ca6-b29f-f5dcb25ee07a/r
---

---
level: 2
---

# Runnable Map

å½“ Runnable å¯¹è±¡æ¥æ”¶çš„è¾“å…¥éœ€è¦è¢«é¢„å¤„ç†æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ä¸€ä¸ª Map ç»“æ„æä¾›æ”¯æŒ

- Map ä¸­çš„æ¯ä¸ªå±æ€§éƒ½æ¥æ”¶ç›¸åŒçš„å‚æ•°ï¼Œä½¿ç”¨è¿™äº›å‚æ•°æ¥è°ƒç”¨å„ä¸ªå±æ€§å¯¹åº”çš„ Runnable å¯¹è±¡æˆ–å‡½æ•°
- è°ƒç”¨çš„è¿”å›å€¼ç”¨äºå¡«å…… Map å¯¹è±¡ï¼Œç„¶åå°†è¯¥å¯¹è±¡ä¼ é€’åˆ°åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ª Runnable å¯¹è±¡

```ts {9-11,14-18|14-22|all} {maxHeight:'75%'}
import { PromptTemplate } from "langchain/prompts";
import { RunnableSequence } from "langchain/schema/runnable";
import { StringOutputParser } from "langchain/schema/output_parser";
import { OpenAI } from "langchain/chat_models/openai";

const prompt1 = PromptTemplate.fromTemplate(`What is the city {person} is from? Only respond with city name.`);
const prompt2 = PromptTemplate.fromTemplate(`What country is the city {city} in? Respond in {language}.`);

/* æ„å»ºç¬¬ä¸€ä¸ª Chain */
const model = new OpenAI({});
const chain = prompt1.pipe(model).pipe(new StringOutputParser());

const combinedChain = RunnableSequence.from([
  /* å°†ç¬¬ä¸€ä¸ª Chain çš„è¿”å›å€¼å’ŒåŸå§‹è¾“å…¥ä¸­çš„éƒ¨åˆ†æ•°æ®è¿›è¡Œç»„åˆ */
  {
    city: chain,
    language: (input) => input.language,
  },
  /* ç»„åˆåçš„å¯¹è±¡æˆä¸ºç¬¬äºŒä¸ª Chain çš„è¾“å…¥ */
  prompt2,
  model,
  new StringOutputParser(),
]);

const result = await combinedChain.invoke({ person: "Chairman Mao", language: "Chinese" });
```

---
layout: iframe
url: https://smith.langchain.com/public/a013b8e0-bf9a-44d2-be05-d67975b8812b/r
---

---
level: 2
---

# Runnable Passthrough

å½“æˆ‘ä»¬è¦ä»ä¸€ä¸ªé•¿é•¿çš„ Runnable Sequence ä¸­æå–æœ€å¤´éƒ¨çš„è¾“å…¥æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ Passthrough é€ä¼ æœºåˆ¶

```ts {11-17|24,27-33|all} {maxHeight:'80%'}
import { ChatOpenAI } from "langchain/chat_models/openai";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { PromptTemplate } from "langchain/prompts";
import { RunnableSequence, RunnablePassthrough } from "langchain/schema/runnable";
import { StringOutputParser } from "langchain/schema/output_parser";
import { Document } from "langchain/document";

const model = new ChatOpenAI({});

/* ä¸º RAG åœºæ™¯å‡†å¤‡ Retriever */
const vectorStore = await HNSWLib.fromTexts(
  ["mitochondria is the powerhouse of the cell"],
  [{ id: 1 }],
  new OpenAIEmbeddings()
);
const retriever = vectorStore.asRetriever();

const prompt = PromptTemplate.fromTemplate(`Answer the question based only on the following context:
{context}

Question: {question}`);

const serializeDocs = (docs: Document[]) => docs.map((doc) => doc.pageContent).join("\n");

const chain = RunnableSequence.from([
  /* æ ¹æ® RAG æç¤ºè¯æ¨¡ç‰ˆçš„éœ€æ±‚å‡†å¤‡ä¸Šä¸‹æ–‡å’Œé—®é¢˜ */
  {
    /* å°† Retriever è¿”å›çš„ä¸€ç»„æ–‡æ¡£æ‹¼æ¥ä¸ºä¸Šä¸‹æ–‡æ®µè½ */
    context: retriever.pipe(serializeDocs),
    /* é€šè¿‡ Passthrough é€ä¼ è·å–åŸå§‹è¾“å…¥ */
    question: new RunnablePassthrough(),
  },
  prompt,
  model,
  new StringOutputParser(),
]);

const result = await chain.invoke("What is the powerhouse of the cell?");
```

---
layout: iframe
url: https://smith.langchain.com/public/3fbc6896-9fbf-4555-aa39-a08095ce1834/r
---

---
level: 2
---

# Tool ä¹Ÿæ˜¯ä¸€ç§ Runnable å¯¹è±¡

å› æ­¤ Tool æ˜¯å¯ä»¥åœ¨ Runnable Sequence ä¸­è¢«ä¸²è”çš„

```ts {12-16|all}
import { SerpAPI } from "langchain/tools";
import { OpenAI } from "langchain/llm/anthropic";
import { PromptTemplate } from "langchain/prompts";
import { StringOutputParser } from "langchain/schema/output_parser";

const prompt = PromptTemplate.fromTemplate(`Turn the following user input into a search query for a search engine:

{input}`);

const model = new OpenAI({});

/* å‡†å¤‡ Web Search å·¥å…· */
const search = new SerpAPI();

/* ç›´æ¥å°† Web Search å·¥å…·ä¼ å…¥å¹¶è°ƒç”¨ */
const chain = prompt.pipe(model).pipe(new StringOutputParser()).pipe(search);

const result = await chain.invoke({ input: "Who is the current prime minister of Malaysia?" });
```

---
layout: iframe
url: https://smith.langchain.com/public/6a5c08ee-96d8-4c97-8cfc-038990c7260f/r
---

---
level: 2
---

# åœ¨ Runnable Sequence ä¸­å¼•å…¥ Memory

Memory å¯ä»¥è¢«æ·»åŠ åˆ°ä»»ä½• Runnable Sequence ä¸­ï¼Œæœ¬è´¨å’Œå°±æ˜¯ç”Ÿæˆä¸Šä¸‹æ–‡æ–‡æœ¬å¡«å……åˆ°æç¤ºè¯ä¸­

```ts {13-19|22-31|39-40|all} {maxHeight:'80%'}
import { ChatPromptTemplate, MessagesPlaceholder } from "langchain/prompts";
import { RunnableSequence } from "langchain/schema/runnable";
import { ChatAnthropic } from "langchain/chat_models/anthropic";
import { BufferMemory } from "langchain/memory";

const model = new ChatAnthropic();
const prompt = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful chatbot"],
  new MessagesPlaceholder("history"),
  ["human", "{input}"],
]);

/* åˆå§‹åŒ– Memory å¯¹è±¡ */
const memory = new BufferMemory({
  returnMessages: true,
  inputKey: "input",      // by default
  outputKey: "output",    // by default
  memoryKey: "history",   // by default
});

const chain = RunnableSequence.from([
  /* ç¬¬ä¸€ä¸ª Map ç”¨äºé€ä¼ åŸå§‹è¾“å…¥å’ŒåŠ è½½ Memory å¯¹è±¡ */ 
  {
    input: (initialInput) => initialInput.input,
    memory: () => memory.loadMemoryVariables({}),
  }, 
  /* ç¬¬äºŒä¸ª Map ç”¨äºç»§ç»­é€ä¼ åŸå§‹è¾“å…¥å’ŒåŠ è½½ Memory çš„ history éƒ¨åˆ†æ•°æ® */ 
  {
    input: (previousOutput) => previousOutput.input,
    history: (previousOutput) => previousOutput.memory.history,
  },
  prompt,
  model,
]);

const inputs = { input: "Hey, I'm Bob!" };
const response = await chain.invoke(inputs);

/* å®Œæˆä¸€æ¬¡å¯¹è¯åè¿›è¡Œ Memory æ•°æ®æ•°æ® */
await memory.saveContext(inputs, { output: response.content });
/*
  {
    history: [
      HumanMessage {
        content: "Hey, I'm Bob!",
        additional_kwargs: {}
      },
      AIMessage {
        content: " Hi Bob, nice to meet you! I'm Claude, an AI assistant created by Anthropic to be helpful, harmless, and honest.",
        additional_kwargs: {}
      }
    ]
  }
*/

const inputs2 = { input: "What's my name?" };
const response2 = await chain.invoke(inputs2);
```

---
level: 2
---

# ä¸º Runnable Sequence æä¾›æ¡ä»¶è·¯ç”±

è·¯ç”±å…è®¸æ‚¨åˆ›å»ºéç¡®å®šæ€§çš„ Chainï¼Œå…¶ä¸­ä¸Šä¸€æ­¥çš„è¾“å‡ºå®šä¹‰äº†ä¸‹ä¸€æ­¥çš„è·¯ç”±æ–¹å‘

åœ¨ JS/TS ä¸­å¯ä½¿ç”¨ [`RunnableBranch`](https://js.langchain.com/docs/expression_language/how_to/routing) æ¥æ„å»ºæ¡ä»¶è·¯ç”±ï¼Œè·¯ç”±ä¼šä¾æ¬¡åŒ¹é…æ¡ä»¶ç›´è‡³é€‰æ‹©é»˜è®¤ Runnable è¿”å›ï¼š

```ts {23-36|43-44|all} {maxHeight:'75%'}
const langChainChain = PromptTemplate.fromTemplate(
  `You are an expert in langchain. Always answer questions starting with "As Harrison Chase told me".
Respond to the following question:

Question: {question}
Answer:`
).pipe(model);

const anthropicChain = PromptTemplate.fromTemplate(
  `You are an expert in anthropic. Always answer questions starting with "As Dario Amodei told me". 
Respond to the following question:

Question: {question}
Answer:`
).pipe(model);

const generalChain = PromptTemplate.fromTemplate(`Respond to the following question:

Question: {question}
Answer:`
).pipe(model);

/* æ„å»ºä¸€ä¸ªæ¡ä»¶è·¯ç”±ï¼Œä¾æ¬¡é€‰æ‹© Anthropic -> LangChain -> General Chain */
const branch = RunnableBranch.from([
  [
    (x: { topic: string; question: string }) =>
      x.topic.toLowerCase().includes("anthropic"),
    anthropicChain, 
  ],
  [
    (x: { topic: string; question: string }) =>
      x.topic.toLowerCase().includes("langchain"),
    langChainChain,
  ],
  generalChain,
]);

const fullChain = RunnableSequence.from([
  {
    topic: classificationChain,
    question: (input: { question: string }) => input.question,
  },
  /* å¯ä»¥ç›´æ¥å°†è·¯ç”±ä¸²è”è¿›ä¸€ä¸ª Runnable Sequence ä¸­ */
  branch,
]);
```

---
level: 2
---

# Runnable çš„ Fallback æœºåˆ¶

[Fallback](https://js.langchain.com/docs/guides/fallbacks) æœºåˆ¶å¯ä»¥æä¾›ä¼˜é›…çš„å¼‚å¸¸å¤„ç†ï¼Œé€šè¿‡â€œä»¥ä¼˜å……æ¬¡â€çš„æ–¹å¼å°½å¯èƒ½ä¿éšœé“¾è·¯æ‰§è¡Œç»§ç»­æ‰§è¡Œ

åœ¨ JS/TS ä¸­ï¼Œå¯ä½¿ç”¨ `runnable.withFallbacks({ fallbacks: [runnable] })` æ¥æ„å»º Fallback æ”¯æ’‘

```ts
const fakeOpenAIModel = new ChatOpenAI({
  modelName: "potato!",   // Fake model name will always throw an error
  maxRetries: 0,
});

const modelWithFallback = fakeOpenAIModel.withFallbacks({
  fallbacks: [new ChatAnthropic({})],
});
```

ä¸ä»…å¯ä»¥æ”¯æ’‘ Runnable å¯¹è±¡ï¼Œè¿˜å¯ä»¥ç›´æ¥ Fallback æ•´ä¸ª Runnable Sequenceï¼š

```ts
const badChain = chatPrompt.pipe(fakeOpenAIChatModel).pipe(outputParser);
const goodChain = prompt.pipe(openAILLM).pipe(outputParser);

const chain = badChain.withFallbacks({
  fallbacks: [goodChain],
});
```

---
layout: quote
---

# Chains

LangChain ä¸ºâ€œé“¾å¼â€åº”ç”¨æä¾›äº† Chain æ¥å£

---
level: 2
---

# é“¾è·¯è°ƒè¯•çš„â€œæ–‡éŸ¬æ­¦ç•¥â€

é™¤äº†æ¥å…¥å¼ºåŠ›çš„ LangSmithï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ Console è¾“å‡ºè°ƒè¯•ä¿¡æ¯

å’Œè°ƒè¯• Runnable Sequence ä¸€æ ·ï¼Œé¦–é€‰çš„è°ƒè¯•æ–¹å¼è‚¯å®šè¿˜æ˜¯æ¥å…¥ LangSmithï¼Œæœ‰ä¸¤ç§æ–¹å¼ï¼š

```sh
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
export LANGCHAIN_API_KEY=<your-api-key>  # still in closed beta
export LANGCHAIN_PROJECT=<your-project>  # if not specified, defaults to "default"
```
```ts {4-}
import { Client } from "langsmith";
import { LangChainTracer } from "langchain/callbacks";

const client = new Client({ apiUrl: "https://api.smith.langchain.com", apiKey: "YOUR_API_KEY" });
const tracer = new LangChainTracer({ projectName: "YOUR_PROJECT_NAME", client });

await model.invoke("Hello, world!", { callbacks: [tracer] })
```

é€€è€Œæ±‚å…¶æ¬¡ï¼Œå¯ä»¥å¼€å¯ `verbose` é€‰é¡¹ï¼Œæœ‰ä¸¤ç§æ–¹å¼ï¼š

```sh
export LANGCHAIN_VERBOSE=true
```
```ts
const chain = new ConversationChain({ llm: chat, verbose: true });
```

---
level: 2
---

# ä¸€æ¡é“¾è·¯è´¯å¤©åœ°

ç±»ä¼¼ Runnable Sequence çš„é“¾è·¯æ„å»ºï¼Œä½†ç›¸æ¯”ä¹‹ä¸‹ [Seqential](https://js.langchain.com/docs/modules/chains/foundational/sequential_chains) æ“ä½œçš„å¤æ‚åº¦æ›´åŠ é«˜

- `SimpleSequentialChain`ï¼šæœ€ç®€å•å½¢å¼ï¼Œæ¯ä¸ªæ­¥éª¤éƒ½æœ‰å•ä¸ªè¾“å…¥/è¾“å‡ºï¼Œä¸€ä¸ªæ­¥éª¤çš„è¾“å‡ºæ˜¯ä¸‹ä¸€ä¸ªçš„è¾“å…¥
- `SequentialChain`ï¼šé¡ºåºé“¾çš„æ›´é€šç”¨å½¢å¼ï¼Œå…è®¸å¤šä¸ªè¾“å…¥/è¾“å‡º

```ts {12-13,21-22|24-|all} {maxHeight:'70%'}
import { SequentialChain, LLMChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";

// This is an LLMChain to write a synopsis given a title of a play and the era it is set in.
const llm = new OpenAI({ temperature: 0 });
const template = `You are a playwright. Given the title of play and the era it is set in, it is your job to write a synopsis for that title.

  Title: {title}
  Era: {era}
  Playwright: This is a synopsis for the above play:`;
const promptTemplate = new PromptTemplate({template, inputVariables: ["title", "era"] });
const synopsisChain = new LLMChain({ llm, prompt: promptTemplate, outputKey: "synopsis" });

// This is an LLMChain to write a review of a play given a synopsis.
const reviewTemplate = `You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.
  
  Play Synopsis:
  {synopsis}
  Review from a New York Times play critic of the above play:`;
const reviewPromptTemplate = new PromptTemplate({ template: reviewTemplate, inputVariables: ["synopsis"] });
const reviewChain = new LLMChain({ llm, prompt: reviewPromptTemplate, outputKey: "review" });

const overallChain = new SequentialChain({
  chains: [synopsisChain, reviewChain],     // ä¸²è”ä¸¤ä¸ª LLMChain
  inputVariables: ["era", "title"],         // å®šä¹‰éœ€è¦ä¼ å…¥çš„å˜é‡
  outputVariables: ["synopsis", "review"],  // å®šä¹‰éœ€è¦è¾“å‡ºçš„å˜é‡
});
const chainExecutionResult = await overallChain.call({ title: "Tragedy at sunset on the beach", era: "Victorian England" });
```

---
layout: iframe
url: https://smith.langchain.com/public/63376249-1360-4cea-9886-da72b3abbfd4/r
---

---
level: 2
---

# ä¸¤ç±»æ¨¡å‹èˆå‰æ²¿

æœ€åŸºç¡€çš„é“¾å¼è°ƒç”¨æ˜¯ [`LLMChain`](https://js.langchain.com/docs/modules/chains/foundational/llm_chain)ï¼Œå®ƒåŒæ—¶æ”¯æŒ LLM å’Œ Chat æ¨¡å‹

```ts
/* æ„å»ºåŸºäº LLM æ¨¡å‹çš„ LLMChain */
const model = new OpenAI({ temperature: 0 });
const prompt = PromptTemplate.fromTemplate("What is a good name for a company that makes {product}?");
const chainA = new LLMChain({ llm: model, prompt });

// é€šè¿‡ .call(object) è¿”å›è¾“å‡ºå¯¹è±¡
const resA = await chainA.call({ product: "colorful socks" });  // { text: '\n\nSocktastic!' }
// é€šè¿‡ .run(string) è¿”å›è¾“å‡ºå­—ç¬¦ä¸²
const resA2 = await chainA.run("colorful socks");   // '\n\nSocktastic!'
```
<br/>

```ts
/* æ„å»ºåŸºäº Chat æ¨¡å‹çš„ LLMChain */
const chat = new ChatOpenAI({ temperature: 0 });
const chatPrompt = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant that translates {input_language} to {output_language}."],
  ["human", "{text}"],
]);
const chainB = new LLMChain({ prompt: chatPrompt, llm: chat });

const resB = await chainB.call({ input_language: "English", output_language: "French", text: "I love programming." });
// { text: "J'adore la programmation." }
```

---
level: 2
---

# ä¸‰ç§å·§æ€æ²»æ–‡æ¡£

LangChain æä¾›å¤„ç†æ–‡æ¡£çš„å·¥å…·é“¾ï¼Œå®ƒä»¬å¯¹äºæ€»ç»“æ–‡æ¡£ã€å›ç­”æ–‡æ¡£é—®é¢˜ã€ä»æ–‡æ¡£ä¸­æå–ä¿¡æ¯ç­‰å¾ˆæœ‰ç”¨

[ä¸‰ç§æ–‡æ¡£å¤„ç†å·¥å…·é“¾](https://js.langchain.com/docs/modules/chains/document/) çš„ä½¿ç”¨æ–¹å¼åŸºæœ¬æ˜¯ä¸€æ ·çš„ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```ts {10-}
import { OpenAI } from "langchain/llms/openai";
import { loadQAStuffChain, loadQAMapReduceChain } from "langchain/chains";
import { Document } from "langchain/document";

const docs = [
  new Document({ pageContent: "Harrison went to Harvard." }),
  new Document({ pageContent: "Ankush went to Princeton." }),
];

/* æ„å»ºå¹¶ä½¿ç”¨ StuffDocumentsChain */
const llmA = new OpenAI({});
const chainA = loadQAStuffChain(llmA);
const resA = await chainA.call({ input_documents: docs, question: "Where did Harrison go to college?" });

/* æ„å»ºå¹¶ä½¿ç”¨ MapReduceChain */
const llmB = new OpenAI({ maxConcurrency: 10 });
const chainB = loadQAMapReduceChain(llmB);
const resB = await chainB.call({ input_documents: docs, question: "Where did Harrison go to college?" });
```

---
level: 3
---

<img src="https://js.langchain.com/assets/images/stuff-818da4c66ee17911bc8861c089316579.jpg" width="400" />
<img src="https://js.langchain.com/assets/images/map_reduce-c65525a871b62f5cacef431625c4d133.jpg" width="800" />

---
level: 3
---

# å±‚å é€’è¿›çš„ Refine æ¨¡å¼

- <B>å¯¹äºæ¯ä¸ªæ–‡æ¡£ï¼Œå®ƒå°†æ‰€æœ‰éæ–‡æ¡£è¾“å…¥ã€å½“å‰æ–‡æ¡£ä»¥åŠæœ€æ–°çš„ä¸­é—´ç­”æ¡ˆä¼ é€’ç»™ LLM é“¾ä»¥è·å¾—æ–°çš„ç­”æ¡ˆ</B>
- ç”±äº Refine é“¾ä¸€æ¬¡ä»…å°†å•ä¸ªæ–‡æ¡£ä¼ é€’ç»™ LLMï¼Œå› æ­¤å¾ˆé€‚åˆéœ€è¦åˆ†æçš„æ–‡æ¡£æ•°é‡å¤šäºæ¨¡å‹ä¸Šä¸‹æ–‡çš„ä»»åŠ¡
- æ˜æ˜¾çš„ç¼ºç‚¹æ˜¯ï¼šå°†æ¯” Stuff æ–‡æ¡£é“¾è¿›è¡Œæ›´å¤šçš„ LLM è°ƒç”¨ï¼›å½“æ–‡æ¡£é¢‘ç¹åœ°äº’ç›¸äº¤å‰å¼•ç”¨æ—¶å¾ˆå¯èƒ½è¡¨ç°ä¸ä½³

![](https://js.langchain.com/assets/images/refine-a70f30dd7ada6fe5e3fcc40dd70de037.jpg)

---
level: 3
---

# ğŸŒ° Refine æ–‡æ¡£é“¾çš„äºŒé˜¶æ®µæç¤ºè¯åº”ç”¨

```ts {8-18|20-34|36-39|all} {maxHeight:'90%'}
import { loadQARefineChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { PromptTemplate } from "langchain/prompts";

/* æœ€ç»ˆæé—®æ—¶ä½¿ç”¨çš„æç¤ºè¯ */
export const questionPromptTemplateString = `Context information is below.
---------------------
{context}
---------------------
Given the context information and no prior knowledge, answer the question: {question}`;

const questionPrompt = new PromptTemplate({
  inputVariables: ["context", "question"],
  template: questionPromptTemplateString,
});

/* ä¸­é—´ Refine è¿‡ç¨‹ä½¿ç”¨çš„æç¤ºè¯ */
const refinePromptTemplateString = `The original question is as follows: {question}
We have provided an existing answer: {existing_answer}
We have the opportunity to refine the existing answer
(only if needed) with some more context below.
------------
{context}
------------
Given the new context, refine the original answer to better answer the question.
You must provide a response, either original answer or refined answer.`;

const refinePrompt = new PromptTemplate({
  inputVariables: ["question", "existing_answer", "context"],
  template: refinePromptTemplateString,
});

/* æ„å»º Refine æ–‡æ¡£é“¾ï¼Œå¹¶å¯¼å…¥ä¸¤ä»½æç¤ºè¯æ¨¡æ¿ */
const embeddings = new OpenAIEmbeddings();
const model = new OpenAI({ temperature: 0 });
const chain = loadQARefineChain(model, { questionPrompt, refinePrompt });

// Load the documents and create the vector store
const loader = new TextLoader("./state_of_the_union.txt");
const docs = await loader.loadAndSplit();
const store = await MemoryVectorStore.fromDocuments(docs, embeddings);

// Select the relevant documents
const question = "What did the president say about Justice Breyer";
const relevantDocs = await store.similaritySearch(question);

const res = await chain.call({ input_documents: relevantDocs, question });
```

---
level: 2
---

# å››å¥—åº”ç”¨å±•å¨åŠ›ï¼šRetrieval QA

[Retrival QA Chain](https://js.langchain.com/docs/modules/chains/popular/vector_db_qa) é€šè¿‡ä»æ£€ç´¢å™¨æ£€ç´¢æ–‡æ¡£ï¼Œç„¶åä½¿ç”¨æ–‡æ¡£å·¥å…·é“¾ï¼Œæ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£å›ç­”é—®é¢˜

```ts {18-22|24-|all} {maxHeight:'85%'}
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain, loadQAStuffChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { PromptTemplate } from "langchain/prompts";
import * as fs from "fs";

const promptTemplate = `Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

{context}

Question: {question}
Answer in Italian:`;
const prompt = PromptTemplate.fromTemplate(promptTemplate);
const model = new OpenAI({});

/* æ„å»ºæ–‡æ¡£æ£€ç´¢å™¨ */
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

/* æ„å»ºåŸºäº Stuff æ–‡æ¡£é“¾å’Œè‡ªå®šä¹‰æç¤ºè¯çš„ Retrieval QA Chain */
const chain = new RetrievalQAChain({
  combineDocumentsChain: loadQAStuffChain(model, { prompt }),
  retriever: vectorStore.asRetriever(),
  returnSourceDocuments: true,  // è¦æ±‚ä¸€å¹¶è¿”å›åŸå§‹æ–‡æ¡£
});
const res = await chain.call({ query: "What did the president say about Justice Breyer?" });
```

---
level: 2
---

# å››å¥—åº”ç”¨å±•å¨åŠ›ï¼šConverational Retrieval QA

[Conversational Retrieval QA Chain](https://js.langchain.com/docs/modules/chains/popular/chat_vector_db) å»ºç«‹åœ¨ Retrieval QA Chain çš„åŸºç¡€ä¸Šï¼Œå¹¶æ¥å…¥ Memory ç»„ä»¶

å®ƒé¦–å…ˆå°†èŠå¤©å†å²è®°å½•å’Œé—®é¢˜ç»„åˆæˆä¸€ä¸ªç‹¬ç«‹çš„é—®é¢˜ï¼Œç„¶åå°†æ£€ç´¢å¾—åˆ°çš„æ–‡æ¡£å’Œé—®é¢˜ä¼ é€’ç»™é—®ç­”é“¾ä»¥è¿”å›å“åº”

```ts {35-49|51-|all} {maxHeight:'75%'}
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationalRetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { BufferMemory } from "langchain/memory";

const CUSTOM_QUESTION_GENERATOR_CHAIN_PROMPT = `Given the following conversation and a follow up question, return the conversation history excerpt that includes any relevant context to the question if it exists and rephrase the follow up question to be a standalone question.
Chat History:
{chat_history}
Follow Up Input: {question}
Your answer should follow the following format:
\`\`\`
Use the following pieces of context to answer the users question.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
----------------
<Relevant chat history excerpt as context here>
Standalone question: <Rephrased question here>
\`\`\`
Your answer:`;

const model = new ChatOpenAI({ temperature: 0 });

const vectorStore = await HNSWLib.fromTexts(
  [
    "Mitochondria are the powerhouse of the cell",
    "Foo is red",
    "Bar is red",
    "Buildings are made out of brick",
    "Mitochondria are made of lipids",
  ],
  [{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],
  new OpenAIEmbeddings()
);

/* æ„å»º Conversational Retrieval QA Chain */
const chain = ConversationalRetrievalQAChain.fromLLM(
  model,
  vectorStore.asRetriever(),
  /* å¯¼å…¥ Memory å’Œè‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿ */
  {
    memory: new BufferMemory({
      memoryKey: "chat_history",
      returnMessages: true,
    }),
    questionGeneratorChainOptions: {
      template: CUSTOM_QUESTION_GENERATOR_CHAIN_PROMPT,
    },
  }
);

const res1 = await chain.call({ question: "I have a friend called Bob. He's 28 years old. He'd like to know what the powerhouse of the cell is?" });
// { text: "The powerhouse of the cell is the mitochondria." }

const res2 = await chain.call({ question: "How old is Bob?" });
// { text: "Bob is 28 years old." }
```

---
layout: iframe
url: https://smith.langchain.com/public/517b2110-349f-4089-8051-9b5203e34c0b/r
---

---
layout: iframe
url: https://smith.langchain.com/public/69a66e55-e00d-4aae-9953-8e42f4feaeea/r
---

---
level: 2
---

# å››å¥—åº”ç”¨å±•å¨åŠ›ï¼šSQL QA

åˆ©ç”¨ LLM çš„ SQL ç”Ÿæˆèƒ½åŠ›ï¼Œå¯ä»¥æ„å»º [SQL Chain](https://js.langchain.com/docs/modules/chains/popular/sqlite) æ¥è¿›è¡Œé¢å‘æ•°æ®åº“çš„é—®ç­”

```ts {24-26|28-36|all} {maxHeight:'80%'}
import { DataSource } from "typeorm";
import { OpenAI } from "langchain/llms/openai";
import { SqlDatabase } from "langchain/sql_db";
import { SqlDatabaseChain } from "langchain/chains/sql_db";
import { PromptTemplate } from "langchain/prompts";

const template = `Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.
Use the following format:

Question: "Question here"
SQLQuery: "SQL Query to run"
SQLResult: "Result of the SQLQuery"
Answer: "Final answer here"

Only use the following tables:

{table_info}

If someone asks for the table foobar, they really mean the employee table.

Question: {input}`;
const prompt = PromptTemplate.fromTemplate(template);

/* æ„å»ºæ•°æ®åº“è¿æ¥ */
const appDataSource = new DataSource({ type: "sqlite", database: "data/Chinook.db" });
const db = await SqlDatabase.fromDataSourceParams({ appDataSource });

/* æ„å»ºå¹¶æ‰§è¡Œ SQL QA Chain */
const chain = new SqlDatabaseChain({
  llm: new OpenAI({ temperature: 0 }),
  database: db,
  sqlOutputKey: "sql",  // è¦æ±‚è¿”å› SQL è¯­å¥
  prompt,               // ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯
});
await chain.call({ query: "How many employees are there in the foobar table?" });
/*
  {
    result: ' There are 8 employees in the foobar table.',
    sql: ' SELECT COUNT(*) FROM Employee;'
  }
*/
```

---
layout: iframe
url: https://smith.langchain.com/public/89aaa750-4115-4ad0-a987-39ea0b2e52f4/r
---

---
level: 3
---

# ç”¨ Runnable Sequence å®ç° SQL QA

```ts {38-51} {maxHeight:'90%'}
import { DataSource } from "typeorm";
import { SqlDatabase } from "langchain/sql_db";
import { RunnableSequence } from "langchain/schema/runnable";
import { PromptTemplate } from "langchain/prompts";
import { StringOutputParser } from "langchain/schema/output_parser";
import { ChatOpenAI } from "langchain/chat_models/openai";

const datasource = new DataSource({ type: "sqlite", database: "Chinook.db" });
const db = await SqlDatabase.fromDataSourceParams({ appDataSource: datasource });

const prompt =
  PromptTemplate.fromTemplate(`Based on the table schema below, write a SQL query that would answer the user's question:
{schema}

Question: {question}
SQL Query:`);

const model = new ChatOpenAI();

const sqlQueryGeneratorChain = RunnableSequence.from([
  {
    schema: async () => db.getTableInfo(),
    question: (input: { question: string }) => input.question,
  },
  prompt,
  model.bind({ stop: ["\nSQLResult:"] }),
  new StringOutputParser(),
]);

const finalResponsePrompt =
  PromptTemplate.fromTemplate(`Based on the table schema below, question, sql query, and sql response, write a natural language response:
{schema}

Question: {question}
SQL Query: {query}
SQL Response: {response}`);

const fullChain = RunnableSequence.from([
  {
    question: (input) => input.question,
    query: sqlQueryGeneratorChain,
  },
  {
    schema: async () => db.getTableInfo(),
    question: (input) => input.question,
    query: (input) => input.query,
    response: (input) => db.run(input.query),
  },
  finalResponsePrompt,
  model,
]);

const finalResponse = await fullChain.invoke({ question: "How many employees are there?" });
```

---
layout: iframe
url: https://smith.langchain.com/public/6d79dcfb-e501-45f5-ad31-0d605233627b/r
---

---
level: 2
---

# å››å¥—åº”ç”¨å±•å¨åŠ›ï¼šWeb API

[API Chain](https://js.langchain.com/docs/modules/chains/popular/api) å…è®¸ä½¿ç”¨ LLM ä¸ API äº¤äº’ä»¥æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œé€šè¿‡æä¾›ä¸æä¾›çš„ API æ–‡æ¡£ç›¸å…³çš„é—®é¢˜æ¥æ„å»ºé“¾

```ts {34-} {maxHeight:'90%'}
import { OpenAI } from "langchain/llms/openai";
import { APIChain } from "langchain/chains";

const OPEN_METEO_DOCS = `BASE URL: https://api.open-meteo.com/

API Documentation
The API endpoint /v1/forecast accepts a geographical coordinate, a list of weather variables and responds with a JSON hourly weather forecast for 7 days. Time always starts at 0:00 today and contains 168 hours. All URL parameters are listed below:

Parameter	Format	Required	Default	Description
latitude, longitude	Floating point	Yes		Geographical WGS84 coordinate of the location
hourly	String array	No		A list of weather variables which should be returned. Values can be comma separated, or multiple &hourly= parameter in the URL can be used.
daily	String array	No		A list of daily weather variable aggregations which should be returned. Values can be comma separated, or multiple &daily= parameter in the URL can be used. If daily weather variables are specified, parameter timezone is required.
current_weather	Bool	No	false	Include current weather conditions in the JSON output.
temperature_unit	String	No	celsius	If fahrenheit is set, all temperature values are converted to Fahrenheit.
windspeed_unit	String	No	kmh	Other wind speed speed units: ms, mph and kn
precipitation_unit	String	No	mm	Other precipitation amount units: inch
timeformat	String	No	iso8601	If format unixtime is selected, all time values are returned in UNIX epoch time in seconds. Please note that all timestamp are in GMT+0! For daily values with unix timestamps, please apply utc_offset_seconds again to get the correct date.
timezone	String	No	GMT	If timezone is set, all timestamps are returned as local-time and data is returned starting at 00:00 local-time. Any time zone name from the time zone database is supported. If auto is set as a time zone, the coordinates will be automatically resolved to the local time zone.
past_days	Integer (0-2)	No	0	If past_days is set, yesterday or the day before yesterday data are also returned.
start_date
end_date	String (yyyy-mm-dd)	No		The time interval to get weather data. A day must be specified as an ISO8601 date (e.g. 2022-06-30).
models	String array	No	auto	Manually select one or more weather models. Per default, the best suitable weather models will be combined.

Variable	Valid time	Unit	Description
temperature_2m	Instant	Â°C (Â°F)	Air temperature at 2 meters above ground
snowfall	Preceding hour sum	cm (inch)	Snowfall amount of the preceding hour in centimeters. For the water equivalent in millimeter, divide by 7. E.g. 7 cm snow = 10 mm precipitation water equivalent
rain	Preceding hour sum	mm (inch)	Rain from large scale weather systems of the preceding hour in millimeter
showers	Preceding hour sum	mm (inch)	Showers from convective precipitation in millimeters from the preceding hour
weathercode	Instant	WMO code	Weather condition as a numeric code. Follow WMO weather interpretation codes. See table below for details.
snow_depth	Instant	meters	Snow depth on the ground
freezinglevel_height	Instant	meters	Altitude above sea level of the 0Â°C level
visibility	Instant	meters	Viewing distance in meters. Influenced by low clouds, humidity and aerosols. Maximum visibility is approximately 24 km.`;

const model = new OpenAI({ modelName: "text-davinci-003" });
const chain = APIChain.fromLLMAndAPIDocs(model, OPEN_METEO_DOCS, { headers: {} });
await chain.call({ question: "What is the weather like right now in Munich, Germany in degrees Farenheit?" });
```

---
layout: iframe
url: https://smith.langchain.com/public/8d7555a1-66bc-4f03-843e-261a4f576f4d/r
---

---
level: 2
---

# ç§ç±»ç¹å¤šçš„â€œé¢„åˆ¶èœâ€

ç›¸æ¯” Runnable Sequence çš„è‡ªå®šä¹‰èƒ½åŠ›å¼ºï¼ŒChain çš„æœ€å¤§ç‰¹ç‚¹å°±æ˜¯é¢„åˆ¶åŒ–ç¨‹åº¦é«˜

> ä¸‹é¢ä»¥ JS/TS SDK å±•ç¤ºä¸€äº›æœ‰ç‰¹è‰²çš„ Chain

<br/>

- åŸºäº [OpenAI Functions](https://platform.openai.com/docs/guides/gpt/function-calling) èƒ½åŠ›åº•åº§çš„ Chainï¼š
  - [`createExtractionChainFromZod`](https://js.langchain.com/docs/modules/chains/additional/openai_functions/extraction)ï¼šä»è¾“å…¥æ–‡æœ¬å’Œæ‰€éœ€ä¿¡æ¯çš„æ¨¡å¼ä¸­æå–å¯¹è±¡åˆ—è¡¨
  - [`createTaggingChain`](https://js.langchain.com/docs/modules/chains/additional/openai_functions/tagging)ï¼šæ ¹æ®æ¨¡å¼ä¸­å®šä¹‰çš„å±æ€§æ¥æ ‡è®°è¾“å…¥æ–‡æœ¬
  - [`createOpenAPIChain`](https://js.langchain.com/docs/modules/chains/additional/openai_functions/openapi)ï¼šåŸºäº Open API è§„èŒƒè‡ªåŠ¨é€‰æ‹©å’Œè°ƒç”¨ API
- [`OpenAIModerationChain`](https://js.langchain.com/docs/modules/chains/additional/moderation)ï¼šå®¡æ ¸é“¾åŸºäº OpenAI Moderation å¯¹äºæ£€æµ‹å¯èƒ½ä»‡æ¨ã€æš´åŠ›ç­‰çš„æ–‡æœ¬å¾ˆæœ‰ç”¨
- [`ConstitutionalChain`](https://js.langchain.com/docs/modules/chains/additional/constitutional_chain)ï¼šè‡ªæˆ‘æ‰¹åˆ¤é“¾æ˜¯ä¸€æ¡ç¡®ä¿è¯­è¨€æ¨¡å‹çš„è¾“å‡ºéµå®ˆä¸€ç»„é¢„å®šä¹‰å‡†åˆ™çš„é“¾

æ­¤å¤–ï¼Œä¹Ÿæä¾›ç”¨äºæ¡ä»¶è·¯ç”±çš„ Chainï¼š
- [`MultiPromptChain`](https://js.langchain.com/docs/modules/chains/additional/multi_prompt_router)ï¼šä½¿ç”¨å¤šæç¤ºé“¾åˆ›å»ºä¸€ä¸ªé—®ç­”é“¾ï¼Œé€‰æ‹©ä¸ç»™å®šé—®é¢˜æœ€ç›¸å…³çš„æç¤ºå¹¶è¿›è¡Œå›ç­”
- [`MultiRetrievalQAChain`](https://js.langchain.com/docs/modules/chains/additional/multi_retrieval_qa_router)ï¼šè¯¥é“¾é€‰æ‹©ä¸ç»™å®šé—®é¢˜æœ€ç›¸å…³çš„ Retrieval QA Chainï¼Œç„¶åä½¿ç”¨å®ƒå›ç­”é—®é¢˜

---
src: ../../pages/common/refs.md
---

---
src: ../../pages/common/end.md
---
